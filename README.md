## 社内FAQチャットボット (RAG, LangChain, LangSmith, Amazon Bedrock)
## プロジェクト概要

本プロジェクトは、従来のシステムエンジニアとして培ってきた知識と経験に加え、  
LLMエンジニアとしてのキャリアチェンジを目指すための学習成果を示すポートフォリオとして独学で開発しました。  
LangChainによるLLMアプリの開発、Amazon BedrockでのLLMの利用、そしてLangSmithを用いたLLMの評価・改善を組み合わせています。  
LLM＋RAGを使用し、社内ドキュメント (会社概要、給与計算規則、勤怠管理マニュアル) を参照して質問応答を行います。

## 技術スタック

-   **言語：**  
    -   Python 3.11.6
-   **フレームワーク：**
    -   LangChain 0.2.17
    -   FastAPI 0.100.0
-   **ライブラリ：**
    -   Pandas 2.1.2
    -   LangChain-Community 0.2.19
    -   LangChain-AWS 0.1.18
    -   LangChain-Core 0.2.43
    -   AWS Lambda Powertools 2.36.0
    -   Boto3 1.34.131
    -   Uvicorn 0.23.2
    -   Python-dotenv 1.0.0
    -   Datasets 2.15.0
    -   NLTK 3.8.1
    -   PyPDF 3.15.1
-   **インフラ：** AWS (Lambda, S3, Bedrock, IAM)
-   **エディタ：** Cursor
-   **コード管理：** Git, GitHub
-   **チャットUI：** Streamlit 1.27.2
-   **埋め込みモデル：** Titan Text Embeddings v2
-   **ベクトルDB：** Amazon OpenSearch Serverless
-   **言語モデル：** Claude 3.5 Sonnet 20241022 v2
-   **LLM評価：** Ragas 0.2.0, LangSmith 0.1.112

## ディレクトリ構成
```
├── internal_faq_chatbot/
│   ├── documents/                               # 関連ドキュメント
│   │   ├── 会社概要.pdf
│   │   ├── 給与計算規則.pdf
│   │   └── 勤怠管理マニュアル.pdf
│   ├── evaluations/                             # 評価関連
│   │   ├── data/                                # 評価データ
│   │   │   └── langsmith_test_questions.json
│   │   ├── metrics/                             # 評価スクリプト
│   │   │   ├── langsmith_evaluation.py
│   │   │   └── ragas_evaluation.py
│   │   └── ragas_results/                       # 評価結果
│   │       └── ragas_evaluation_results.json
│   ├── fastapi/                                 # チャットの送受信
│   │   └── fastapi_app.py
│   ├── lambda_functions/                        # Lambda関数
│   │   └── document_search/                     # ドキュメント検索
│   │       ├── bedrock_kb_search_function.py
│   │       └── requirements.txt
│   ├── streamlit/                               # 画面UI
│   │   └── streamlit_app.py
│   ├── .gitignore                               # Git管理から除外
│   ├── README.md                                # Readmeファイル
│   └── requirements.txt                         # プロジェクト全体の依存関係

```
## AWSアーキテクチャー図
![image](https://github.com/user-attachments/assets/d4d13cd0-d1d4-4f2d-9ada-b89555583d5b)

## 動作手順

1.  **(ドキュメントの情報の格納)**
    -   事前にドキュメントをチャンク分割し、ベクトル化してベクトルDBにインデックス化して格納。  
        一連の作業はAmazon Bedrock Knowledge Baseで行う。
2.  **(質問)**
    -   ユーザーの質問を送信し、アプリケーション 側で受信。
3.  **(ベクトル検索)**
    -   作成済みのインデックスを参照して検索を行い、質問に関連する情報を取得。
4.  **(質問と関連情報を元にAIに問い合わせ)**
    -    ユーザーの質問と取得した関連情報を組み合わせて、言語モデルにプロンプトを送信。
5.  **(回答生成)**
    -   質問と関連情報を元に回答を生成。
6.  **(回答)**
    -  生成された回答をユーザーに返信。

## デモ

(ここにデモのスクリーンショット挿入)
<!-- ![デモ画像1](./path/to/your/demo_image1.png) -->
<!-- ![デモ画像2](./path/to/your/demo_image2.png) -->

## 評価
1.  **(RAG性能評価)**
    -   作成したRAGシステムをテストデータ(質問と想定回答の組み合わせ)を用意してRagas、LangSmithで性能評価する。
    #### Ragas評価結果 
    - **忠実性（Faithfulness）**: 85.3%  
    - **回答関連性（Answer Relevancy）**: 46.7%  
    - **コンテキスト精度（Context Precision）**: 84.9%

    #### LangSmith評価結果
    - **実行回数**: 20回
    - **エラー率**: 0%
    - **レイテンシー**:
      - P50: 3.10秒
      - P99: 9.02秒
    - **トークン数**: 26,623トークン
    　![image](https://github.com/user-attachments/assets/34ef59bf-7ec9-4619-9d52-28d254737fdf)
    　![image](https://github.com/user-attachments/assets/9520815d-4243-47b5-b464-5f49cd1086b8)


    #### 性能考察
    **Ragas評価分析**:
    - **忠実性（85.3%）**: 関連ドキュメントの情報に忠実な回答ができており、ハルシネーションが少ない
    - **回答関連性（46.7%）**: 質問に対する回答の関連性が低く、改善の余地が大きい
    - **コンテキスト精度（85.0%）**: 適切なコンテキスト選択ができている

    **LangSmith評価分析**:
    - **レイテンシ**: P50は3.10秒と許容範囲だが、P99が9.02秒と大幅に遅延
    - **トークン効率**: 26,623トークンと多量のトークンを消費している
    - **安定性**: エラー率0%と安定した動作を実現

    **課題と改善策**:
    1.  **回答関連性の低さ**: プロンプトエンジニアリングによる質問理解力の向上
    2.  **レイテンシの不安定さ**: Lambda関数の見直しとチャンキング改善
    3.  **トークン消費量**: チャンキングによる効率的なドキュメント選択
    
2.  **(LLM・RAGの精度改善)**
    -   Ragas 、LangSmithで性能評価した結果を元に分析してプロンプトエンジニアリング・RAGの精度改善を実施。再テストして前回と比べて精度が上がっているか検証する。

    ### 【プロンプトエンジニアリング改善】
    
    **改善前の課題**:
    - 回答関連性が46.7%と低い
    - 質問に直接関係ない情報も回答に含まれる
    - プロンプトの指示が曖昧で一貫性に欠ける

    **実装した改善策**:
    1. **役割の明確化**: 「株式会社架空ソリューションズの社内FAQチャットボット」として明確な役割を定義
    2. **3段階の回答プロセス**: 質問分析→関連箇所特定→回答作成の明確な手順を指示
    3. **厳守事項の設定**: 関連ドキュメント外の情報使用禁止、無関係情報の除外を明確化
    4. **構造化プロンプト**: セクション分けにより指示を整理し、理解しやすい形式に改善

    ### 改善効果の確認

    #### Ragas評価結果 
    - **忠実性（Faithfulness）**: 86.7%  
    - **回答関連性（Answer Relevancy）**: 63.9% (17%向上)
    - **コンテキスト精度（Context Precision）**: 84.9%

     **具体的な改善**:
    ```
    **テスト質問**: 「本社の所在地を教えてください」
    
    改善前: 「本社は〒100-0001 東京都千代田区架空町1-1-1 架空ビル10階にあります。
           代表電話番号は03-9876-5432、FAXは03-9876-5431です。」
    
    改善後: 「本社の所在地は〒100-0001 東京都千代田区架空町1-1-1 架空ビル10階です。」
    ```

    ![image](https://github.com/user-attachments/assets/ffcb6d48-1951-4f6f-ac4a-6111b38ed157)

    これらの改善により、質問と回答の関連性向上とハルシネーション抑制を実現しました。

    ### 【RAGの精度改善】

    #### ①Lambda関数の見直し

    **改善前の課題**:
    - Lambda関数が4つに分散し、複雑な処理フローを形成
    - レイテンシの不安さとコスト効率の悪化

    **実装した改善策**:  
    下記、Lambda関数の削除
    1. 質問文のチャンク分割**： 質問文のチャンク分割は不適切と理解。チャンク分割は関連ドキュメントに適用すべき処理
    2. 質問文のベクトル化**： Bedrock Knowledge Baseが質問文の自動ベクトル化に対応しており不要
    3. PDFテキストの前処理**： Bedrock Knowledge BaseがPDF処理（テキスト化→チャンク分割→ベクトル化→DB保存）を一括対応

    #### 改善効果の確認

    **システム効率の向上**:
    - **Lambda呼び出し回数**: 4回 → 1回（75%削減）
    - **運用関数数**: 4関数 → 1関数（75%削減）
    - **システム複雑性**: 大幅に簡素化

    **性能向上の具体例**:

    - 改善前: 複数Lambda関数による多段階処理  
      質問文 → PDF前処理 → 質問文前処理 → ベクトル化 → Bedrock Knowledge Base検索 → 回答生成

    - 改善後: 統合処理による直接的なフロー  
      質問文 → Bedrock Knowledge Base検索 → 回答生成

    **技術的メリット**:
    - **レイテンシの短縮**: 不要な処理ステップ削除により応答時間改善
    - **回答精度の向上**: 不適切な前処理削除により情報損失を回避
    - **運用コストの削減**: Lambda関数数削減によるコスト効率化
    - **保守性の向上**: シンプルなアーキテクチャによる運用負荷軽減

    この改善により、シンプルで効率的なRAGシステムを構築。  

    #### ②チャンキング戦略
    **改善前の課題**:
    - プロンプトエンジニアリング改善によるトークン消費量の増加
    - 不適切なチャンキングによる、レイテンシの不安定さ

    **実装した改善策**:
    1. **チャンキング設定の最適化**:  
    　- AWSデフォルトチャンキング（チャンクサイズ：300、オーバーラップ：20%）  
    　- ↓  
    　- 固定サイズチャンキング（チャンクサイズ：200、オーバーラップ：10%）に変更
    2. **複数パターンの性能テスト**: チャンクサイズ150〜250、オーバーラップ10%〜20%の組み合わせで最適設定を検証

    **チャンキング設定**:
    - **採用設定**: チャンクサイズ200、オーバーラップ10%
    - **採用理由**: チャンクサイズ150ではコンテキスト不足によりRagasメトリクスが低下。バランスの良い性能を示した設定を採用

    **LangSmith評価結果**:
    - **実行回数**: 20回
    - **エラー率**: 0%
    - **レイテンシー**:
    - P50: 2.81秒（-0.29秒改善）
    - P99: 5.44秒（-3.58秒改善）
    - **トークン数**: 26,077トークン（546トークン削減）

    ![image](https://github.com/user-attachments/assets/34ef59bf-7ec9-4619-9d52-28d254737fdf)
　  ![image](https://github.com/user-attachments/assets/d601f70a-fef3-4719-bdff-cdd28b7afb2c)

    **技術的メリット**:
    - **レイテンシの大幅改善**: P99で約40%の応答時間短縮を実現
    - **トークン効率化**: チャンクサイズ最適化により無駄なトークン消費を削減
    - **コンテキスト品質維持**: 適切なサイズ設定により情報の一貫性を保持

    この改善により、レスポンス性能の大幅向上とコスト効率化を同時に達成しました。

## 評価結果  
```
・Ragas評価結果 
    忠実性（Faithfulness）: 85.3% → 86.7% (+1％)
    回答関連性（Answer Relevancy）: 46.7% → 63.9％ (+17％)
    コンテキスト精度（Context Precision）: 84.9% → 84.9％ (+0％)

・LangSmith評価結果
    実行回数 : 20回
    エラー率 : 0%
    レイテンシー :
    P50: 3.10秒 → 2.81秒 (-0.29秒改善)
    P99: 9.02秒 → 5.44秒（-3.58秒改善）
    トークン数 : 26,623トークン → 26,077トークン (546トークン削減)
```
## 苦労した点
(ここに苦労した点や工夫した内容を記載)

## ポートフォリオ作成によって得た経験、スキル

*   RAGシステムの開発・評価・改善の一連サイクルの実践経験
*   Anthropicが提供するClaude APIの利用経験
*   StreamlitのPocフレームワークの開発経験
*   LangChainのLLMフレームワークの開発経験
*   FastAPIを用いたAPIの開発経験
*   AWS (Lambda、S3、Bedrock、IAM) でのシステム構築経験
*   チャンキング、エンべディング、ベクトルDBの経験
*   Rages、LangSmithを利用したLLMの性能評価経験
*   プロンプトエンジニアリング、RAGの精度改善経験


